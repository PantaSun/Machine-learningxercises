{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯\n",
    "### 0.前言\n",
    "> 在机器学习中，朴素贝叶斯分类器是一系列以假设特征之间强（朴素）独立下运用贝叶斯定理为基础的简单概率分类器。\n",
    "朴素贝叶斯自20世纪50年代已广泛研究。在20世纪60年代初就以另外一个名称引入到文本信息检索界中，并仍然是文本分类的一种热门（基准）方法，文本分类是以词频为特征判断文件所属类别或其他（如垃圾邮件、合法性、体育或政治等等）的问题。通过适当的预处理，它可以与这个领域更先进的方法（包括支持向量机）相竞争。它在自动医疗诊断中也有应用。\n",
    "\n",
    ">朴素贝叶斯分类器是高度可扩展的，因此需要数量与学习问题中的变量（特征/预测器）成线性关系的参数。最大似然训练可以通过评估一个封闭形式的表达式来完成，只需花费线性时间，而不需要其他很多类型的分类器所使用的费时的迭代逼近。\n",
    "\n",
    ">在统计学和计算机科学文献中，朴素贝叶斯模型有各种名称，包括简单贝叶斯和独立贝叶斯。所有这些名称都参考了贝叶斯定理在该分类器的决策规则中的使用，但朴素贝叶斯不（一定）用到贝叶斯方法；《Russell和Norvig》提到“‘朴素贝叶斯’有时被称为贝叶斯分类器，这个马虎的使用促使真正的贝叶斯论者称之为傻瓜贝叶斯模型。”\n",
    "\n",
    ">朴素贝叶斯是一种构建分类器的简单方法。该分类器模型会给问题实例分配用特征值表示的类标签，类标签取自有限集合。它不是训练这种分类器的单一算法，而是一系列基于相同原理的算法：所有朴素贝叶斯分类器都假定样本每个特征与其他特征都不相关。举个例子，如果一种水果其具有红，圆，直径大概3英寸等特征，该水果可以被判定为是苹果。尽管这些特征相互依赖或者有些特征由其他特征决定，然而朴素贝叶斯分类器认为这些属性在判定该水果是否为苹果的概率分布上独立的。\n",
    "\n",
    ">对于某些类型的概率模型，在监督式学习的样本集中能获取得非常好的分类效果。在许多实际应用中，朴素贝叶斯模型参数估计使用最大似然估计方法；换而言之，在不用到贝叶斯概率或者任何贝叶斯模型的情况下，朴素贝叶斯模型也能奏效。\n",
    "----摘自[维基百科](https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 收集数据\n",
    "以在线社区的留言板为例，如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当，用1表示；正常留言用0表示。下文中，使用的一篇文档就相当于一条留言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 准备数据：从文本中构建词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_set():\n",
    "    \"\"\"创建实验样本，假设这六个列表为六篇文章\"\"\"\n",
    "    posting_list = [\n",
    "        ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "        ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "        ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "        ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "        ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "        ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']\n",
    "    ]\n",
    "    class_vec = [0, 1, 0, 1, 0, 1] # 1代表含有侮辱性文字的言论；0代表正常的言论\n",
    "    return posting_list, class_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vocab_list(data_set):\n",
    "    \"\"\"创建一个包含在所有文档中不重复出现的词的列表\"\"\"\n",
    "    vocab_set = set([])\n",
    "    for document in data_set:\n",
    "        vocab_set = vocab_set | set(document)  # 创建并集\n",
    "    return list(vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def words_set2vec(vocab_list, input_set):\n",
    "    \"\"\"将一篇文档中所有不重复的词转换为0-1向量\"\"\"\n",
    "    return_vec = [0] * len(vocab_list)\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            return_vec[vocab_list.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return return_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posting_list, class_vec = load_data_set()\n",
    "posting_list\n",
    "class_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['food', 'ate', 'cute', 'worthless', 'mr', 'buying', 'dalmation', 'maybe', 'has', 'problems', 'stop', 'please', 'love', 'is', 'steak', 'posting', 'how', 'to', 'stupid', 'licks', 'quit', 'flea', 'my', 'him', 'park', 'dog', 'not', 'I', 'help', 'so', 'garbage', 'take']\n"
     ]
    }
   ],
   "source": [
    "my_vocab_list = create_vocab_list(posting_list)\n",
    "print(my_vocab_list) # 可以看到此表中没有重复的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"检查my_vocab_list中每个词在第一篇（索引为0）文章中是否出现\"\"\"\n",
    "my_vec = words_set2vec(my_vocab_list, posting_list[0]) \n",
    "print(my_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"检查my_vocab_list中每个词在第4篇（索引为3）文章中是否出现\"\"\"\n",
    "my_vec = words_set2vec(my_vocab_list, posting_list[3]) \n",
    "print(my_vec)\n",
    "# len(my_vec)\n",
    "# import numpy as np\n",
    "# a = np.zeros(len(my_vec))\n",
    "# print(a + my_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到my_vocab_list中的第四个词‘problems’是在第一篇文章中出现，所以被标记为“1”。\n",
    "##### 将所有文档都转化为词向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "train_mat = []\n",
    "for doc in posting_list:\n",
    "    train_mat.append(words_set2vec(my_vocab_list, doc))\n",
    "print(train_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.分析数据\n",
    "   首先我们要知道训练数据中每篇文档（留言）是属于哪种类型？侮辱性或非侮辱性。然后我们计算词向量中每个词在已知文档分类的条件下，该词出现的概率和侮辱性文档占总文档的概率。最后根据贝叶斯准则，见下式，计算出我们想要的结果。\n",
    "   设w为一个向量，它由多个数值组成，数值个数与词向量中的词个数相同。\n",
    "$$ p(c_i|w) = \\cfrac{p(w|c_i)p(c_i)}{p(w)}$$\n",
    "$c_i$表示第$i$个分类。\n",
    "\n",
    "在计算$p(w|c_i)$时，要用到朴素贝叶斯中的“朴素”二字，就是假设$w$中的每一个值都是相互独立的，那么$p(w|c_i)=p(w_0,w_1,...,w_N|c_i)$就等于$$\\prod_{j=0}^{N}p(w_j|c_i) = p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)...p(w_N|c_i), i=0,1$$ $w_j$表示$w$中第$j+1$个值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.训练算法：从词向量训练算法\n",
    "##### 算法伪码：\n",
    "    计算每个类别中文档的个数\n",
    "    对每篇训练文档：\n",
    "        对每个类别：\n",
    "            如果词条出现在文档中->增加该词条的计数值\n",
    "            增加所有词条的计数值\n",
    "    对每个类别：\n",
    "         对每个词条：\n",
    "             将该词条的数目除以总词条数目得到概率\n",
    "    返回每个类别的条件概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 获取留言的数目，共6条\n",
    "num_train_docs = len(train_mat)\n",
    "num_train_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取词向量的长度，32\n",
    "num_words = len(train_mat[0])\n",
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 侮辱性留言占总留言数的比例\n",
    "pAbusive = sum(class_vec)/float(num_train_docs) \n",
    "\n",
    "# 存放非侮辱性留言（即类型为0）中每个词出现的个数\n",
    "p0num = np.zeros(num_words) \n",
    "# 存放侮辱性留言（即类型为1）中每个词出现的个数\n",
    "p1num = np.zeros(num_words)\n",
    "\n",
    "# 存放类型为0的所有留言中所有词的出现的总次数\n",
    "p0Denom = 0.0\n",
    "# 存放类型为1的所有留言中所有词的出现的总次数\n",
    "p1Denom = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  0.,  1.,  1.,  0.,  1.,  0.,  1.,  3.,  2.,  0.,  1.,\n",
       "        0.,  1.,  1.,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.,  2.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,\n",
       "        0.,  0.,  1.,  0.,  1.,  3.,  0.,  1.,  0.,  0.,  1.,  1.,  2.,\n",
       "        1.,  0.,  0.,  0.,  1.,  1.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(num_train_docs): # 对留言矩阵进行循环\n",
    "        if class_vec[i] == 1:  # 如果该留言类别为1，即侮辱性留言\n",
    "            p1num += train_mat[i]  \n",
    "            p1Denom += sum(train_mat[i])\n",
    "        else:\n",
    "            p0num += train_mat[i]\n",
    "            p0Denom += sum(train_mat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  0.,  0.,  2.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  1.,  0.,  1.,  3.,  0.,  1.,  0.,  0.,  1.,  1.,  2.,\n",
       "         1.,  0.,  0.,  0.,  1.,  1.]), 19.0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1num, p1Denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  0.,  1.,  1.,  0.,  1.,  0.,  1.,  3.,  2.,  0.,  1.,\n",
       "         0.,  1.,  1.,  1.,  0.,  0.]), 24.0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0num, p0Denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算在类型为1时，词向量中每个词出现的概率\n",
    "p1vec = p1num / p1Denom\n",
    "# 计算在类型为0时，词向量中每个词出现的概率\n",
    "p0vec = p0num / p0Denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.05263158,  0.        ,  0.        ,  0.10526316,  0.        ,\n",
       "         0.05263158,  0.        ,  0.05263158,  0.        ,  0.        ,\n",
       "         0.05263158,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.05263158,  0.        ,  0.05263158,  0.15789474,  0.        ,\n",
       "         0.05263158,  0.        ,  0.        ,  0.05263158,  0.05263158,\n",
       "         0.10526316,  0.05263158,  0.        ,  0.        ,  0.        ,\n",
       "         0.05263158,  0.05263158]),\n",
       " array([ 0.        ,  0.04166667,  0.04166667,  0.        ,  0.04166667,\n",
       "         0.        ,  0.04166667,  0.        ,  0.04166667,  0.04166667,\n",
       "         0.04166667,  0.04166667,  0.04166667,  0.04166667,  0.04166667,\n",
       "         0.        ,  0.04166667,  0.04166667,  0.        ,  0.04166667,\n",
       "         0.        ,  0.04166667,  0.125     ,  0.08333333,  0.        ,\n",
       "         0.04166667,  0.        ,  0.04166667,  0.04166667,  0.04166667,\n",
       "         0.        ,  0.        ]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1vec, p0vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"将上述过程合成一个函数\"\"\"\n",
    "import numpy as np\n",
    "def trainNB0(train_mat, train_category): \n",
    "    num_train_docs = len(train_mat) # 文档数目，即网站上的留言数目\n",
    "    num_words = len(train_mat[0])\n",
    "    pAbusive = sum(train_category)/float(num_train_docs) # 侮辱性留言占总留言数的比例\n",
    "    p0num = np.zeros(num_words) \n",
    "    p1num = np.zeros(num_words)\n",
    "    p0Denom = 0.0\n",
    "    p1Denom = 0.0\n",
    "    \n",
    "    for i in range(num_train_docs): # 对留言矩阵进行循环\n",
    "        if train_category[i] == 1:  # 如果该留言类别为1，即侮辱性留言\n",
    "            p1num += train_mat[i]  \n",
    "            p1Denom += sum(train_mat[i])\n",
    "        else:\n",
    "            p0num += train_mat[i]\n",
    "            p0Denom += sum(train_mat[i])\n",
    "    p1vec = p1num / p1Denom\n",
    "    p0vec = p0num / p0Denom\n",
    "        \n",
    "    return p1vec, p0vec, pAbusive\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用该函数\n",
    "p1vec, p0vec, pAbusive = trainNB0(train_mat, class_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05263158,  0.        ,  0.        ,  0.10526316,  0.        ,\n",
       "        0.05263158,  0.        ,  0.05263158,  0.        ,  0.        ,\n",
       "        0.05263158,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.05263158,  0.        ,  0.05263158,  0.15789474,  0.        ,\n",
       "        0.05263158,  0.        ,  0.        ,  0.05263158,  0.05263158,\n",
       "        0.10526316,  0.05263158,  0.        ,  0.        ,  0.        ,\n",
       "        0.05263158,  0.05263158])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.04166667,  0.04166667,  0.        ,  0.04166667,\n",
       "        0.        ,  0.04166667,  0.        ,  0.04166667,  0.04166667,\n",
       "        0.04166667,  0.04166667,  0.04166667,  0.04166667,  0.04166667,\n",
       "        0.        ,  0.04166667,  0.04166667,  0.        ,  0.04166667,\n",
       "        0.        ,  0.04166667,  0.125     ,  0.08333333,  0.        ,\n",
       "        0.04166667,  0.        ,  0.04166667,  0.04166667,  0.04166667,\n",
       "        0.        ,  0.        ])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.测试算法\n",
    "    测试前要对算法进行改进，因为在计算概率相乘，如果其中某个概率为0，那么最后的结果也就是0，就无法进行相应的分类了，因此将所有的词出现的次数初始值设为1，并将分母初始化为2。\n",
    "   \n",
    "     还有一个问题就是下溢出，由于大部分概率比较小，计算机在进行四舍五入时，最后的结果可能得到0，于是我们采用log的形式，根据高等数学可知，有如下式子。\n",
    "$$ln(a*b) = ln(a) + lin(b)$$\n",
    "    \n",
    "    所以将那些概率先进性取对数，再相加，就可以避免下溢出，于是就有了朴素贝叶斯分类函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"修改函数\"\"\"\n",
    "import numpy as np\n",
    "def trainNB0(train_mat, train_category): \n",
    "    num_train_docs = len(train_mat) # 文档数目，即网站上的留言数目\n",
    "    num_words = len(train_mat[0])\n",
    "    pAbusive = sum(train_category)/float(num_train_docs) # 侮辱性留言占总留言数的比例\n",
    "    p0num = np.ones(num_words) \n",
    "    p1num = np.ones(num_words)\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    \n",
    "    for i in range(num_train_docs): # 对留言矩阵进行循环\n",
    "        if train_category[i] == 1:  # 如果该留言类别为1，即侮辱性留言\n",
    "            p1num += train_mat[i]  \n",
    "            p1Denom += sum(train_mat[i])\n",
    "        else:\n",
    "            p0num += train_mat[i]\n",
    "            p0Denom += sum(train_mat[i])\n",
    "    p1vec = np.log(p1num / p1Denom)\n",
    "    p0vec = np.log(p0num / p0Denom)\n",
    "        \n",
    "    return p1vec, p0vec, pAbusive\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 朴素贝叶斯函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyNB(vec2classify, p0vec, p1vec, pclass1):\n",
    "    p1 = sum(vec2classify * p1vec) + np.log(pclass1)\n",
    "    p0 = sum(vec2classify * p0vec) + np.log(pclass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    posting_list, class_list = load_data_set()\n",
    "    my_vocab_list = create_vocab_list(posting_list)\n",
    "    train_mat = []\n",
    "    for doc in posting_list:\n",
    "        train_mat.append(words_set2vec(my_vocab_list, doc))\n",
    "    p1v, p0v, pAb = trainNB0(train_mat, class_list)\n",
    "    test1 = ['love', 'my', 'dalmation']\n",
    "    test1_vec = words_set2vec(my_vocab_list, test1)\n",
    "    print(test1,\"classified as:\",classifyNB(test1_vec, p0v, p1v, pAb))\n",
    "    \n",
    "    test2 = ['stupid', 'garbage']\n",
    "    test2_vec = words_set2vec(my_vocab_list, test2)\n",
    "    print(test2,\"classified as:\",classifyNB(test2_vec, p0v, p1v, pAb))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as: 0\n",
      "['stupid', 'garbage'] classified as: 1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述结果可以看出，分类结果和我们预想的一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*参考文献：《机器学习实战》Peter Harrington*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
