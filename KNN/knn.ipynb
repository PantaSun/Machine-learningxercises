{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn\n",
    "### 0. 简介\n",
    "#### knn算法即k-近邻算法\n",
    "> k-近邻算法 (k-nearest neighbor) 是一种基本分类与回归方法。  ----李航《统计学方法》\n",
    "\n",
    "#### 思想\n",
    "在给定的训练数据集中寻找与新输入的实例最近邻的 k 个实例，统计这 k 个实例所属的分类，则拥有实例个数最多的分类就看做是该新输入实例的分类。k-nn算法没有显示的学习过程，即不需要先训练算法。\n",
    "#### 三要素\n",
    "##### k值的选择\n",
    "##### 距离度量\n",
    "有$x_i, x_j$为 n 维向量，即$x_i, x_j \\in R^n$， 则 $x_i, x_j$ 之间的距离 $L_p$ 定义为：\n",
    "$$L_p(x_i, x_j)  = (\\sum^{n}_{l=1} | x_i^{(l)} - x_j^{(l)}|)^{1/p} $$ \n",
    "- 当 p = 1 时，该距离成为[曼哈顿距离](https://zh.wikipedia.org/wiki/%E6%9B%BC%E5%93%88%E9%A0%93%E8%B7%9D%E9%9B%A2)。\n",
    "- 当 p = 2 时，该距离称为[欧氏距离](https://zh.wikipedia.org/wiki/%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E8%B7%9D%E7%A6%BB)。\n",
    "- 当 p = 无穷大 时， 该距离就是各个坐标距离的最大值。\n",
    "##### 分类决策条件\n",
    "\n",
    "### 1. 算法\n",
    "#### 输入：\n",
    "① 训练数据集 $ T = \\{x_i, y_i\\} $ ,i=1,2,...,N，$x_i$ 为实例的特征向量，$y_i$ 为实例的类别。\n",
    "\n",
    "② 实例特征向量 x\n",
    "#### 输出：实例 x 的类别\n",
    "\n",
    "① 根据距离计算方法，如欧氏距离，计算出里 x 距离最近的 k 个点。\n",
    "\n",
    "② 根据分类决策方法，如多数表决法，最总决定 x 的分类 y 。\n",
    "                                  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
